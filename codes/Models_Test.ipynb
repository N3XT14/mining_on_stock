{"cells":[{"cell_type":"code","source":["#Yash Oswal (yoswal@binghamton.edu)\n","#Praneeth Purini (ppurini@binghamton.edu)"],"metadata":{"id":"QE3lklRX-sTb"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":29084,"status":"ok","timestamp":1683255091732,"user":{"displayName":"Yash Oswal","userId":"08926553451935908326"},"user_tz":240},"id":"84AtUsH0bo3w","outputId":"d9698148-b63a-4103-c6d9-cc1f53eea2bc"},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}],"source":["#Necessary Imports\n","# !pip install ta\n","import numpy as np\n","import pandas as pd\n","import os\n","import pickle\n","import torch\n","# import ta\n","from google.colab import drive\n","drive.mount('/content/drive')\n","\n","#Data Source Location\n","os.chdir(\"/content/drive/My Drive/mining-on-stock\")"]},{"cell_type":"markdown","source":["Task 1 Start"],"metadata":{"id":"OIhsWCVVsp6r"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"Rx0yFSNDgQGy"},"outputs":[],"source":["#Fetching training data from the pkl file.\n","with open('training_set.pkl', 'rb') as train_file:\n","  train_data = pickle.load(train_file)\n","\n","#We can also use pd.read_pickle fuction"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":8768,"status":"ok","timestamp":1683039224455,"user":{"displayName":"Praneeth","userId":"13740106769956375743"},"user_tz":240},"id":"yP7YDvkFhPEU","outputId":"605c5987-0daf-4c99-e73c-e32d6765225f"},"outputs":[{"output_type":"stream","name":"stdout","text":["Threshold 1: -0.002106\n","Threshold 2: 0.001981\n","Total number of data points: 4404000\n","Number of data points in decrease level: 1467857\n","Number of data points in no big change level: 1466290\n","Number of data points in increase level: 1467853\n"]}],"source":["# Concatenate the daily percentage changes for all the stocks into a single list\n","change_list = [df['Close'].pct_change() for df in train_data]\n","all_changes = sorted([item for sublist in change_list for item in sublist])\n","all_changes = np.nan_to_num(all_changes)\n","\n","# Compute the two thresholds that divide the list into three equal-sized intervals\n","level1 = np.percentile(all_changes, 33.33)\n","level2 = np.percentile(all_changes, 66.67)\n","\n","# Assign labels to the daily percentage changes for each stock\n","labels = []\n","for df in train_data:\n","    close_pct_change = df['Close'].pct_change() \n","    label = pd.cut(close_pct_change, bins=[-np.inf, level1, level2, np.inf], labels=['decrease', 'no big change', 'increase'])\n","    labels.append(label)\n","    \n","# Output the thresholds and the number of data points in each level\n","print(f'Threshold 1: {level1:.6f}')\n","print(f'Threshold 2: {level2:.6f}')\n","labels = pd.concat(labels)\n","print(f'Total number of data points: {len(labels)}')\n","print(f'Number of data points in decrease level: {(labels == \"decrease\").sum()}')\n","print(f'Number of data points in no big change level: {(labels == \"no big change\").sum()}')\n","print(f'Number of data points in increase level: {(labels == \"increase\").sum()}')"]},{"cell_type":"markdown","source":["Task 1 End"],"metadata":{"id":"Vcv8s9xRssNU"}},{"cell_type":"markdown","source":["Task 2 Start"],"metadata":{"id":"jsXPdkMrsln0"}},{"cell_type":"code","source":["from features import *\n","def feature_engineering_func():\n","  #These are the functions that we have considered using for making additional features in our dataset. \n","  feature_funcs = [('awesome_oscillator', None),\n","  ('bollinger_percent_b', None),\n","  ('cfo', None),\n","  ('cmo', None),\n","  ('coppock_curve', None),\n","  ('detrended_price_oscillator', None),\n","  ('disparity_index', None),\n","  ('elder_impulse_system', None),\n","  ('gator_oscillator', None), \n","  ('historical_volatility', None),\n","  ('intraday_momentum_index', None),\n","  ('linear_reg_slope', None),\n","  ('macd', None),\n","  ('macd_divergence', None), \n","  ('momentum_indicator', None), \n","  ('qstick', 14), \n","  ('projected_volume_at_time', 14),\n","  ('projected_aggregate_volume', 14),\n","  ('price_volume_trend', None),\n","  ('positive_volume_index', None),\n","  ('twiggs_money_flow', None),\n","  ('volume_underlay', None),\n","  ('on_balance_volume', None),\n","  ('moving_average', 30),\n","  ('parabolic_sar', (0.02, 0.2)),\n","  ('pretty_good_oscillator', (10, 20)),\n","  ('price_momentum_oscillator', (12, 24)),\n","  ('price_oscillator', (12, 26, 9)),\n","  ('price_rate_of_change', 14),\n","  ('KST', (10, 15, 20, 30, 10, 10, 10, 15)),\n","  ('SpecialK', (10, 15, 20, 30, 10, 10, 10, 15)),\n","  ('ravi', (7, 65)),\n","  ('rsi', 14),\n","  ('rsi_divergence', (14, 9)),\n","  ('rainbow_moving_average', 30),\n","  ('rainbow_oscillator', (10, 1.0)),\n","  ('random_walk_index', 14),\n","  ('relative_vigor_index', 14),\n","  ('schaff_trend_cycle', 10),\n","  ('standard_deviation', 14),\n","  ('stochastic_divergence', 14),\n","  ('stochastic_momentum_index', (14, 3)),\n","  ('stochastic_rsi', (14, 3, 3)),\n","  ('stochastics', (14, 3, 3)),\n","  ('trix', 15),\n","  ('trade_volume_index', None),\n","  ('trend_intensity_index', 14),\n","  ('true_range', None),\n","  ('typical_price', None),\n","  ('ultimate_oscillator', (7, 14, 28, 4, 2, 1)),\n","  ('vwap', None),\n","  ('vertical_horizontal_filter', 28),\n","  ('vortex_indicator', 14),\n","  ('williams_r', 14),\n","  ('zigzag', 5),\n","  ('calculate_adx', None), \n","  ('atr_bands', None),\n","  ('accumulative_swing_index', None),\n","  ('alligator', None),\n","  ('aroon', None),\n","  ('aroon_oscillator', None),\n","  ('average_true_range', None),\n","  ('cog', None),\n","  ('cpr', None),\n","  ('chv', None),\n","  ('cmf', None),\n","  ('ci', None),\n","  ('cci', None),\n","  ('donchian_channel', None),\n","  ('ehlers_fisher_transform', None),\n","  ('elder_ray_index', None),\n","  ('fractal_chaos_bands', None),\n","  ('fractal_chaos_oscillator', None),\n","  ('gopalakrishnan_range_index', None),\n","  ('high_minus_low', None),\n","  ('highest_high', None),\n","  ('ichimoku_clouds', None),\n","  ('keltner_channel', None),\n","  ('linear_reg_forecast', None),\n","  ('mass_index', None),\n","  ('median_price', None),\n","  ('money_flow_index', None)]\n","\n","  #There are about 82 functions above from which we have obtained more than 100 new features.\n","  #With careful evaluation, we finally considered using 101 features for task-3 removing all other erratic features.\n","  \n","  # Concatenate the daily percentage changes for all the stocks into a single list\n","  change_list = [df['Close'].pct_change() for df in train_data]\n","  all_changes = sorted([item for sublist in change_list for item in sublist])\n","  all_changes = np.nan_to_num(all_changes)\n","\n","  # Compute the two thresholds that divide the list into three equal-sized intervals\n","  level1 = np.percentile(all_changes, 33.33)\n","  level2 = np.percentile(all_changes, 66.67)\n","\n","  # Assign labels to the daily percentage changes for each stock\n","  labels = []\n","  for df in train_data:\n","      close_pct_change = df['Close'].pct_change() \n","      label = pd.cut(close_pct_change, bins=[-np.inf, level1, level2, np.inf], labels=['decrease', 'no big change', 'increase'])\n","      labels.append(label)\n","      \n","  # Output the thresholds and the number of data points in each level\n","  labels = pd.concat(labels)\n","  \n","  # Define a function to calculate the new features  \n","  def calculate_new_features(df):\n","      df_new = df.copy()\n","      for func in feature_funcs:\n","        func_name = func[0]\n","        func_args = func[1]        \n","        func_module = __import__('features', globals(), locals(), [func_name], 0) # Import the function dynamically using its name\n","        func_to_call = getattr(func_module, func_name)\n","        try:\n","          if func_args is None: \n","              result = func_to_call(df)\n","          elif isinstance(func_args, tuple):\n","              result = func_to_call(df, *func_args)\n","          else:\n","              result = func_to_call(df, func_args)   \n","          if isinstance(result, pd.DataFrame):\n","            for i in result.columns.tolist():\n","              column_name = f'{func_name} ({i})'\n","              df_new[column_name] = result[i]\n","          elif isinstance(result, pd.Series):\n","              column_name = f'{func_name} ({result.name})'\n","              df_new[column_name] = result\n","        except AttributeError:            \n","            print(f\"{func_name} function not found in the features module\")\n","        except Exception as e:\n","            print(f\"An error occurred while calling {func_name}: {str(e)}\")\n","      return df_new\n","\n","  # Create a new list to store the processed dataframes\n","  processed_data = []\n","\n","  # Iterate over each dataframe in the list\n","  for df in train_data.copy():      \n","      df_new = calculate_new_features(df) # Apply the calculate_new_features function to the current dataframe\n","      processed_data.append(df_new) # Append the updated dataframe to the new list\n","  \n","  #Now checking if training data has any infinity values in them \n","  list_inf = []\n","  for i in range(0, len(train_data)):\n","      inf_locs = np.where(np.isinf(train_data[i].iloc[:, :-1]))\n","      list_inf.append((i, inf_locs))  \n","  #Dropping rows that contain nan's in the label column for all the data frames in the train data.\n","  #Those are actually the first rows of each dataframe for which the nan's in the labels caused because of percent change.  \n","  i = 0\n","  while(i < len(train_data)):\n","    train_data[i] = train_data[i].dropna(subset = ['Label'])\n","    i = i +1\n","  #Now each dataframe has 2201 rows.\n","  #Droping 16th column as we dropeed in training\n","  #Droping 16th column as we dropeed in training\n","  i = 0\n","  while(i < len(train_data)):\n","    train_data[i] = train_data[i].drop(train_data[i].columns[16], axis =1)\n","    i = i +1\n","  list_inf = []\n","  for i in range(0, len(train_data)):\n","      inf_locs = np.where(np.isinf(train_data[i].iloc[:, :-1]))\n","      list_inf.append((i, inf_locs))\n","\n","  #16th column is the culprit having inf values\n","  for i, j in list_inf:\n","    if len(j[0])!=0:\n","      k = 0\n","      while(k < len(train_data)):\n","        # select column containing infinite values\n","        col = train_data[k].iloc[:, j[1]]\n","      # replace infinite values with NaN\n","        col.replace([np.inf, -np.inf], np.nan, inplace=True)\n","      # replace NaN values with maximum value of column\n","        max_val = col.max()\n","        col.fillna(max_val, inplace=True)\n","        k = k + 1\n","        #Removing the above obtained features\n","\n","  for i in range(0, len(train_data)):\n","    for k in ['coppock_curve (coppock)','twiggs_money_flow (Twiggs Money Flow)' ,'volume_underlay (Volume Ratio MA)', 'stochastic_rsi (Close)', 'vwap (None)', 'donchian_channel (upper_dc)', 'donchian_channel (lower_dc)', 'ichimoku_clouds (senkou_span_b)']:\n","      train_data[i] = train_data[i].drop(k, axis=1)\n","  #10\n","  for i in range(0, len(train_data)):\n","    for j in ['cfo (cfo)', 'ichimoku_clouds (chikou_span)', 'cci (cci)', 'chv (chv)', 'cmf (cmf)', 'money_flow_index (Money Flow Index)', 'relative_vigor_index (None)']:\n","      train_data[i][j].fillna(method='bfill', inplace=True)\n","    for j in ['KST (Close)', 'SpecialK (Close)', 'calculate_adx (ADX)']:\n","      train_data[i][j].fillna(method='ffill', inplace=True)\n","  #14\n","  for i in range(0, len(train_data)):\n","    for j in range(0, train_data[0].shape[1]):\n","      train_data[i].iloc[:, j].fillna(method='bfill', inplace=True)\n","      train_data[i].iloc[:, j].fillna(method='ffill', inplace=True)\n","      train_data = pd.concat(train_data, axis =0)\n","  return train_data"],"metadata":{"id":"TkUditOAyVoA"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":13,"metadata":{"id":"ckORrrbKbssR","executionInfo":{"status":"ok","timestamp":1683255725107,"user_tz":240,"elapsed":17391,"user":{"displayName":"Yash Oswal","userId":"08926553451935908326"}}},"outputs":[],"source":["#Load Feature Engineered Stock dataset: 100 Features including labels\n","with open('data_with_features_clean_train.pkl', 'rb') as train_file: #To do: Change file name.\n","  train_data = pickle.load(train_file)\n","del train_file"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":369},"executionInfo":{"elapsed":12,"status":"ok","timestamp":1683247633815,"user":{"displayName":"Yash Oswal","userId":"08926553451935908326"},"user_tz":240},"id":"cBVXZ03BYD91","outputId":"ee81ecba-4b42-411e-8842-b24cca80282e"},"outputs":[{"output_type":"stream","name":"stdout","text":["Last 5 datapoints of the first stock among 2000 stocks\n"]},{"output_type":"execute_result","data":{"text/plain":["          Open      High       Low     Close    Volume  \\\n","2197  0.680849  0.683633  0.672997  0.674055  0.351048   \n","2198  0.674389  0.674890  0.671326  0.674055  0.143069   \n","2199  0.674110  0.676227  0.672440  0.676004  0.312048   \n","2200  0.675725  0.675837  0.668820  0.669321  0.279230   \n","2201  0.669293  0.671326  0.662973  0.664198  0.552933   \n","\n","      awesome_oscillator (AO)  bollinger_percent_b (percent_b)  cfo (cfo)  \\\n","2197                 0.010157                         0.744441   0.068595   \n","2198                 0.010020                         0.723160   0.068595   \n","2199                 0.010509                         0.755154   0.068595   \n","2200                 0.008306                         0.536443   0.068595   \n","2201                 0.005024                         0.362143   0.068595   \n","\n","      cmo (cmo)  detrended_price_oscillator (dpo)  ...  \\\n","2197  21.168686                          0.004813  ...   \n","2198  19.300231                          0.005677  ...   \n","2199  21.857938                          0.008792  ...   \n","2200  18.491048                          0.003247  ...   \n","2201  15.904361                         -0.000906  ...   \n","\n","      ichimoku_clouds (senkou_span_a)  ichimoku_clouds (chikou_span)  \\\n","2197                         0.683299                       0.689504   \n","2198                         0.682909                       0.689504   \n","2199                         0.682505                       0.689504   \n","2200                         0.682505                       0.689504   \n","2201                         0.679735                       0.689504   \n","\n","      keltner_channel (Keltner Center)  keltner_channel (Keltner Upper)  \\\n","2197                          0.677062                         0.692473   \n","2198                          0.677206                         0.692105   \n","2199                          0.677414                         0.691979   \n","2200                          0.677475                         0.692413   \n","2201                          0.677201                         0.691448   \n","\n","      keltner_channel (Keltner Lower)  linear_reg_forecast (LRF)  \\\n","2197                         0.661650                   0.689504   \n","2198                         0.662307                   0.689504   \n","2199                         0.662849                   0.689504   \n","2200                         0.662537                   0.689504   \n","2201                         0.662953                   0.689504   \n","\n","      mass_index (Mass_Index)  median_price (Median Price)  \\\n","2197                 1.080827                     0.678315   \n","2198                 0.966600                     0.673108   \n","2199                 0.896525                     0.674333   \n","2200                 0.925453                     0.672328   \n","2201                 0.977753                     0.667149   \n","\n","      money_flow_index (Money Flow Index)          Label  \n","2197                             7.843752       decrease  \n","2198                             7.179743  no big change  \n","2199                             7.179743       increase  \n","2200                             7.535366       decrease  \n","2201                             7.707911       decrease  \n","\n","[5 rows x 101 columns]"],"text/html":["\n","  <div id=\"df-6b9b689d-8603-47cb-8a25-4d3b7fbdc7d8\">\n","    <div class=\"colab-df-container\">\n","      <div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>Open</th>\n","      <th>High</th>\n","      <th>Low</th>\n","      <th>Close</th>\n","      <th>Volume</th>\n","      <th>awesome_oscillator (AO)</th>\n","      <th>bollinger_percent_b (percent_b)</th>\n","      <th>cfo (cfo)</th>\n","      <th>cmo (cmo)</th>\n","      <th>detrended_price_oscillator (dpo)</th>\n","      <th>...</th>\n","      <th>ichimoku_clouds (senkou_span_a)</th>\n","      <th>ichimoku_clouds (chikou_span)</th>\n","      <th>keltner_channel (Keltner Center)</th>\n","      <th>keltner_channel (Keltner Upper)</th>\n","      <th>keltner_channel (Keltner Lower)</th>\n","      <th>linear_reg_forecast (LRF)</th>\n","      <th>mass_index (Mass_Index)</th>\n","      <th>median_price (Median Price)</th>\n","      <th>money_flow_index (Money Flow Index)</th>\n","      <th>Label</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>2197</th>\n","      <td>0.680849</td>\n","      <td>0.683633</td>\n","      <td>0.672997</td>\n","      <td>0.674055</td>\n","      <td>0.351048</td>\n","      <td>0.010157</td>\n","      <td>0.744441</td>\n","      <td>0.068595</td>\n","      <td>21.168686</td>\n","      <td>0.004813</td>\n","      <td>...</td>\n","      <td>0.683299</td>\n","      <td>0.689504</td>\n","      <td>0.677062</td>\n","      <td>0.692473</td>\n","      <td>0.661650</td>\n","      <td>0.689504</td>\n","      <td>1.080827</td>\n","      <td>0.678315</td>\n","      <td>7.843752</td>\n","      <td>decrease</td>\n","    </tr>\n","    <tr>\n","      <th>2198</th>\n","      <td>0.674389</td>\n","      <td>0.674890</td>\n","      <td>0.671326</td>\n","      <td>0.674055</td>\n","      <td>0.143069</td>\n","      <td>0.010020</td>\n","      <td>0.723160</td>\n","      <td>0.068595</td>\n","      <td>19.300231</td>\n","      <td>0.005677</td>\n","      <td>...</td>\n","      <td>0.682909</td>\n","      <td>0.689504</td>\n","      <td>0.677206</td>\n","      <td>0.692105</td>\n","      <td>0.662307</td>\n","      <td>0.689504</td>\n","      <td>0.966600</td>\n","      <td>0.673108</td>\n","      <td>7.179743</td>\n","      <td>no big change</td>\n","    </tr>\n","    <tr>\n","      <th>2199</th>\n","      <td>0.674110</td>\n","      <td>0.676227</td>\n","      <td>0.672440</td>\n","      <td>0.676004</td>\n","      <td>0.312048</td>\n","      <td>0.010509</td>\n","      <td>0.755154</td>\n","      <td>0.068595</td>\n","      <td>21.857938</td>\n","      <td>0.008792</td>\n","      <td>...</td>\n","      <td>0.682505</td>\n","      <td>0.689504</td>\n","      <td>0.677414</td>\n","      <td>0.691979</td>\n","      <td>0.662849</td>\n","      <td>0.689504</td>\n","      <td>0.896525</td>\n","      <td>0.674333</td>\n","      <td>7.179743</td>\n","      <td>increase</td>\n","    </tr>\n","    <tr>\n","      <th>2200</th>\n","      <td>0.675725</td>\n","      <td>0.675837</td>\n","      <td>0.668820</td>\n","      <td>0.669321</td>\n","      <td>0.279230</td>\n","      <td>0.008306</td>\n","      <td>0.536443</td>\n","      <td>0.068595</td>\n","      <td>18.491048</td>\n","      <td>0.003247</td>\n","      <td>...</td>\n","      <td>0.682505</td>\n","      <td>0.689504</td>\n","      <td>0.677475</td>\n","      <td>0.692413</td>\n","      <td>0.662537</td>\n","      <td>0.689504</td>\n","      <td>0.925453</td>\n","      <td>0.672328</td>\n","      <td>7.535366</td>\n","      <td>decrease</td>\n","    </tr>\n","    <tr>\n","      <th>2201</th>\n","      <td>0.669293</td>\n","      <td>0.671326</td>\n","      <td>0.662973</td>\n","      <td>0.664198</td>\n","      <td>0.552933</td>\n","      <td>0.005024</td>\n","      <td>0.362143</td>\n","      <td>0.068595</td>\n","      <td>15.904361</td>\n","      <td>-0.000906</td>\n","      <td>...</td>\n","      <td>0.679735</td>\n","      <td>0.689504</td>\n","      <td>0.677201</td>\n","      <td>0.691448</td>\n","      <td>0.662953</td>\n","      <td>0.689504</td>\n","      <td>0.977753</td>\n","      <td>0.667149</td>\n","      <td>7.707911</td>\n","      <td>decrease</td>\n","    </tr>\n","  </tbody>\n","</table>\n","<p>5 rows × 101 columns</p>\n","</div>\n","      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-6b9b689d-8603-47cb-8a25-4d3b7fbdc7d8')\"\n","              title=\"Convert this dataframe to an interactive table.\"\n","              style=\"display:none;\">\n","        \n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","       width=\"24px\">\n","    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n","    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n","  </svg>\n","      </button>\n","      \n","  <style>\n","    .colab-df-container {\n","      display:flex;\n","      flex-wrap:wrap;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  </style>\n","\n","      <script>\n","        const buttonEl =\n","          document.querySelector('#df-6b9b689d-8603-47cb-8a25-4d3b7fbdc7d8 button.colab-df-convert');\n","        buttonEl.style.display =\n","          google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","        async function convertToInteractive(key) {\n","          const element = document.querySelector('#df-6b9b689d-8603-47cb-8a25-4d3b7fbdc7d8');\n","          const dataTable =\n","            await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                     [key], {});\n","          if (!dataTable) return;\n","\n","          const docLinkHtml = 'Like what you see? Visit the ' +\n","            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n","            + ' to learn more about interactive tables.';\n","          element.innerHTML = '';\n","          dataTable['output_type'] = 'display_data';\n","          await google.colab.output.renderOutput(dataTable, element);\n","          const docLink = document.createElement('div');\n","          docLink.innerHTML = docLinkHtml;\n","          element.appendChild(docLink);\n","        }\n","      </script>\n","    </div>\n","  </div>\n","  "]},"metadata":{},"execution_count":3}],"source":["#Last 5 data points of the first stock\n","print(f'Last 5 datapoints of the first stock among 2000 stocks')\n","stock_len = 2201\n","train_data.iloc[stock_len-5: stock_len] #To do: Once the concatenated file is ready."]},{"cell_type":"markdown","source":["Task 2 End"],"metadata":{"id":"izVOpicMsnUF"}},{"cell_type":"markdown","source":["Custom DataLoader Class to process, load the data in chunks\n","\n","---\n","\n"],"metadata":{"id":"txf_8nGJetv1"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"sTG7-QUDYpcC"},"outputs":[],"source":["import torch\n","from torch.utils.data import DataLoader, Dataset, TensorDataset\n","import multiprocessing\n","\n","class MyDataset(Dataset):\n","    def __init__(self, df, stock_len, seq_length):\n","        self.X = []\n","        self.y = []\n","        \n","        labels = df.iloc[:, -1].values # Extract labels from the last column\n","        cnt = len(labels)\n","        \n","        df = df.iloc[:, :-1] # Remove labels column from the DataFrame        \n","        values = torch.tensor(df.values) # Convert DataFrame to tensor\n","        \n","        # Create input/output sequences\n","        while cnt != 0:\n","          for i in range(seq_length, stock_len):\n","            self.X.append(values[i-seq_length:i, :])\n","            self.y.append(labels[i])\n","          cnt -= stock_len\n","        \n","    def __getitem__(self, idx):\n","        return self.X[idx], self.y[idx]\n","    \n","    def __len__(self):\n","        return len(self.X)\n","\n","def data_generator(train_data, stock_len, seq_length, batch_size, num_workers):\n","    train_dataset = MyDataset(train_data, stock_len, seq_length)\n","    \n","    # Create DataLoader\n","    # Optimzation 1: Dataset and Dataloaders for batch processing\n","    # Optimization 2: (num_of_workers, pin_memory(GPU specific))    \n","    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=num_workers, pin_memory=True)\n","\n","    for X_batch, y_batch in train_loader:\n","        # Convert labels to tensor\n","        y_train = np.array(y_batch)\n","        labels_encoded = torch.zeros((len(y_train), 3), dtype=torch.float32)\n","        labels_encoded[y_train == 'increase', 2] = 1\n","        labels_encoded[y_train == 'no big change', 1] = 1\n","        labels_encoded[y_train == 'decrease', 0] = 1        \n","        y_train_tensor = torch.tensor(labels_encoded, dtype=torch.float32).clone().detach()\n","\n","        yield X_batch, y_train_tensor\n","\n","    # Free up memory\n","    del train_dataset, train_loader\n","    del labels_encoded\n","    del y_train, X_batch, y_batch, y_train_tensor\n","\n","def prepare_train_data(train_data = None, stock_len=2201, seq_length = 100, batch_size = 32):\n","  num_workers=multiprocessing.cpu_count() #parallel computing\n","\n","  # Optimization 4 (Using data generator)\n","  generator = data_generator(train_data, stock_len, seq_length, batch_size, num_workers) \n","\n","  return generator"]},{"cell_type":"code","source":["#num_layers = 1\n","#nn.dropout(p = 0.2)\n","def perform_task_3_train():\n","  for itr in range(0, 10):\n","    class LSTM1(nn.Module):\n","      def __init__(self, input_dim=params_dict[\"input_dim\"][itr], \n","                  hidden_dim_LSTM=params_dict[\"hidden_dim_LSTM\"][itr],\n","                  hidden_dim_FC=params_dict[\"hidden_dim_FC\"][itr],\n","                  output_dim=params_dict[\"output_dim\"][itr],\n","                  num_LSTM_layers=params_dict[\"num_layers_LSTM\"][itr],\n","                  num_layers_FC=params_dict[\"num_layers_FC\"][itr]):\n","          \n","            super(LSTM1, self).__init__()\n","            self.lstm = nn.LSTM(input_dim, hidden_dim_LSTM, num_layers=num_LSTM_layers, batch_first=True) #num_layers = 2\n","            self.dropout = nn.Dropout(p=0.2)\n","            self.fc = nn.Linear(hidden_dim_LSTM, output_dim)\n","            self.softmax = nn.Softmax(dim=1)\n","            self.num_LSTM_layers = num_LSTM_layers\n","            self.hidden_dim_LSTM = hidden_dim_LSTM \n","            \n","            # Use Xavier initialization for weights\n","            init.xavier_uniform_(self.lstm.weight_ih_l0)\n","            init.orthogonal_(self.lstm.weight_hh_l0)\n","            init.constant_(self.lstm.bias_ih_l0, 0.0)\n","            init.constant_(self.lstm.bias_hh_l0, 0.0)\n","            init.xavier_uniform_(self.fc.weight)\n","            init.constant_(self.fc.bias, 0.0)\n","\n","      def forward(self, x):\n","          h0 = torch.zeros(self.num_LSTM_layers, x.size(0), self.hidden_dim_LSTM).requires_grad_().to(device)\n","          c0 = torch.zeros(self.num_LSTM_layers, x.size(0), self.hidden_dim_LSTM).requires_grad_().to(device)\n","          out, (hn, cn) = self.lstm(x, (h0.detach(), c0.detach()))\n","          out = self.dropout(out)\n","          out = self.fc(out[:, -1, :])\n","          out = out.view(-1, 3)\n","          out = self.softmax(out)\n","          return out\n","    # Check if CUDA is available\n","    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","  #--------------------------------------------------------------------------------------------------------------------------------------------  \n","    def train_model1(batch_generator, model_id=params_dict[\"model_id\"][itr], \n","                  input_dim=params_dict[\"input_dim\"][itr], \n","                  learning_rate=params_dict[\"learning rate\"][itr], \n","                  num_epochs=params_dict[\"num_epochs\"][itr], \n","                  hidden_dim_LSTM=params_dict[\"hidden_dim_LSTM\"][itr],\n","                  output_dim=params_dict[\"output_dim\"][itr]):\n","          # Evaluate each model for 10 epochs\n","          # Initialize the model\n","          learning_rate = learning_rate\n","          num_epochs = num_epochs\n","          input_dim = input_dim\n","          hidden_dim_LSTM = hidden_dim_LSTM\n","          # output_dim = y_train_tensor.shape[1]\n","          output_dim = 3\n","\n","          # Define the loss function and optimizer\n","          criterion = nn.CrossEntropyLoss()  \n","\n","          # Initialize the model\n","          print('Model no', model_id)\n","          model = LSTM1(input_dim, hidden_dim_LSTM, output_dim).to(device)\n","\n","          # Define the optimizer and scheduler for the current model\n","          optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n","          scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=10, gamma=0.1)\n","\n","          # Checkpointing parameters for the current model\n","          checkpoint_interval = 1\n","          checkpoint_file = f'lstm_task_3_checkpoint_{model_id}.pth'\n","\n","          # Define early stopping parameters\n","          best_loss = float('inf')\n","          early_stop_counter = 0\n","          early_stop_patience = 5\n","\n","          # Train the current model for 10 epochs\n","          for epoch in range(num_epochs):\n","              # Set model to train mode\n","              model.train()\n","              # model.double()\n","\n","              # Iterate over batches\n","              for x_batch, y_batch in tqdm(generator):\n","                  # Move data to device\n","                  x_batch, y_batch = x_batch.to(device), y_batch.to(device)\n","                  # Cast x_batch to float32\n","                  x_batch = x_batch.float()\n","                  print(x_batch.shape)\n","\n","                  # Forward pass\n","                  outputs = model(x_batch)\n","                  loss = criterion(outputs, y_batch)\n","\n","                  # Backward and optimize\n","                  optimizer.zero_grad()\n","                  loss.backward()\n","                  optimizer.step()\n","\n","              # Print the loss for every 10 epochs\n","              if (epoch + 1) % 10 == 0:\n","                  print('Epoch [{}/{}], Loss: {:.4f}'.format(epoch + 1, num_epochs, loss.item()))\n","\n","              # Check if current model has reached early stopping criteria\n","              print(loss.item())\n","              if loss.item() < best_loss:\n","                  best_loss = loss.item()\n","                  early_stop_counter = 0\n","              else:\n","                  early_stop_counter += 1\n","\n","              if early_stop_counter >= early_stop_patience:\n","                  print(f\"Training for model {itr} stopped early at epoch {epoch+1} due to early stopping\")\n","                  break\n","\n","              # Save checkpoint at regular intervals\n","              if (epoch + 1) % checkpoint_interval == 0:\n","                  torch.save({\n","                      'epoch': epoch + 1,\n","                      'model_state_dict': model.state_dict(),\n","                      'optimizer_state_dict': optimizer.state_dict(),\n","                      'loss': loss.item(),\n","                  }, checkpoint_file)\n","\n","              # Step the scheduler\n","              scheduler.step()\n","\n","          # Delete variables to free up memory\n","          del model, optimizer, scheduler\n","  #------------------------------------------------------------------------------------------------------------------------------------------------------------\n","    # Empty the cache to free up GPU memory\n","    torch.cuda.empty_cache()\n","    seq_length = 100 # Define sequence length\n","    batch_size = 32 # Create data loader\n","    stocks_500 = 2201*500\n","    generator = prepare_train_data(train_data.iloc[:stocks_500, :], 2201, seq_length, batch_size)\n","  #------------------------------------------------------------------------------------------------------------------------------------------------------------\n","    train_model1(generator, itr)\n","# perform_task_3_train()"],"metadata":{"id":"QZSargFL2Z4M"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["num_layers_FC = ([1]*5) + ([2]*5)\n","num_layers_FC.reverse()\n","hidden_dim_LSTM = [i for i in range(21, 31)]\n","hidden_dim_LSTM.reverse()\n","params_dict = {\"input_dim\": [100]*10, \n","               \"hidden_dim_FC\": [i for i in range(21, 31)],\n","               \"output_dim\": [3]*10, \n","               \"num_layers_LSTM\":([1]*5) + ([2]*5), \n","               \"learning rate\":[0.001, 0.01, 0.02, 0.03, 0.04, 0.05, 0.06, 0.07, 0.08, 0.09], \n","               \"num_epochs\": [i for i in range(10, 21)],\n","               \"model_id\":[1, 2, 3, 4, 5, 6, 7, 8, 9, 10] }\n","params_dict[\"num_layers_FC\"] = num_layers_FC\n","params_dict[\"hidden_dim_LSTM\"] = hidden_dim_LSTM "],"metadata":{"id":"R6GmsV4u2N_f","executionInfo":{"status":"ok","timestamp":1683255690603,"user_tz":240,"elapsed":365,"user":{"displayName":"Yash Oswal","userId":"08926553451935908326"}}},"execution_count":11,"outputs":[]},{"cell_type":"code","source":["metrics = []\n","#Load Feature Engineered Stock dataset: 100 Features including labels\n","with open('data_with_features_clean_test.pkl', 'rb') as test_file: #To do: Change file name.\n","  test_data = pickle.load(test_file)\n","del test_file\n","\n","def perform_task_3():\n","  for itr2 in range(0, 10):\n","    class LSTM1(nn.Module):\n","      def __init__(self, input_dim=params_dict[\"input_dim\"][itr2], \n","                  hidden_dim_LSTM=params_dict[\"hidden_dim_LSTM\"][itr2],\n","                  hidden_dim_FC=params_dict[\"hidden_dim_FC\"][itr2],\n","                  output_dim=params_dict[\"output_dim\"][itr2],\n","                  num_LSTM_layers=params_dict[\"num_layers_LSTM\"][itr2],\n","                  num_layers_FC=params_dict[\"num_layers_FC\"][itr2]):\n","          \n","            super(LSTM1, self).__init__()\n","            self.lstm = nn.LSTM(input_dim, hidden_dim_LSTM, num_layers=num_LSTM_layers, batch_first=True) #num_layers = 2\n","            self.dropout = nn.Dropout(p=0.2)\n","            self.fc = nn.Linear(hidden_dim_LSTM, output_dim)\n","            self.softmax = nn.Softmax(dim=1)\n","            self.num_LSTM_layers = num_LSTM_layers\n","            self.hidden_dim_LSTM = hidden_dim_LSTM \n","            \n","            # Use Xavier initialization for weights\n","            init.xavier_uniform_(self.lstm.weight_ih_l0)\n","            init.orthogonal_(self.lstm.weight_hh_l0)\n","            init.constant_(self.lstm.bias_ih_l0, 0.0)\n","            init.constant_(self.lstm.bias_hh_l0, 0.0)\n","            init.xavier_uniform_(self.fc.weight)\n","            init.constant_(self.fc.bias, 0.0)\n","\n","      def forward(self, x):\n","          h0 = torch.zeros(self.num_LSTM_layers, x.size(0), self.hidden_dim_LSTM).requires_grad_().to(device)\n","          c0 = torch.zeros(self.num_LSTM_layers, x.size(0), self.hidden_dim_LSTM).requires_grad_().to(device)\n","          out, (hn, cn) = self.lstm(x, (h0.detach(), c0.detach()))\n","          out = self.dropout(out)\n","          out = self.fc(out[:, -1, :])\n","          out = out.view(-1, 3)\n","          out = self.softmax(out)\n","          return out\n","    # Check if CUDA is available\n","    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","  #------------------------------------------------------------------------------------------------------------------------------------------  \n","    def perform_validation1(num_models=1, file_desc=itr2, data_generator=None):\n","      # Evaluate each model on the primary training set and the validation set  \n","      checkpoint_file = f'lstm_task_3_checkpoint_{file_desc}.pth' # Load the trained model from the checkpoint file\n","      checkpoint = torch.load(checkpoint_file, map_location=torch.device('cpu'))  \n","      model = LSTM1(params_dict[\"input_dim\"][itr2], params_dict[\"hidden_dim_LSTM\"][itr2], 3).to(device)\n","      model.load_state_dict(checkpoint['model_state_dict'])\n","\n","      # Evaluate the current model on the passed data  \n","      result_pred_labels = []  \n","      precisions = []\n","      accuracies = []\n","      for X_data, y_data in data_generator:        \n","        with torch.no_grad():\n","          X_data = X_data.float()\n","          outputs = model.forward(X_data.to(device))\n","\n","        # Convert predicted and true labels to one-hot encodings\n","        predicted_labels = torch.argmax(outputs, dim=1).to(device)\n","        true_labels = torch.argmax(y_data, dim=1).to(device)\n","\n","        # Calculate precision for each class    \n","        for i in range(3):\n","          if i == 2: # positive class\n","              true_positives = torch.sum((predicted_labels == i) & (true_labels == i))\n","              false_positives = torch.sum((predicted_labels == i) & (true_labels != 2))\n","              precision = true_positives.float() / (true_positives + false_positives).float()            \n","              accuracies.append(torch.mean((predicted_labels == true_labels).float() * (true_labels == i).float()))\n","          else: # negative class\n","              true_negatives = torch.sum((predicted_labels == i) & (true_labels == i) & (true_labels != 2))\n","              false_positives = torch.sum((predicted_labels == i) & (true_labels != i) & (true_labels != 2))\n","              precision = true_negatives.float() / (true_negatives + false_positives).float()\n","              accuracies.append(torch.mean((predicted_labels == true_labels).float() * (true_labels != i).float()))\n","          if not torch.isnan(precision):        \n","            precisions.append(precision)\n","      # Calculate average precision and accuracy\n","      avg_precision = torch.mean(torch.tensor(precisions))\n","      avg_accuracy = torch.mean(torch.tensor(accuracies))\n","\n","      # Calculate percentage of positive predictions\n","      percent_positive = torch.mean((predicted_labels == 2).float())\n","      \n","      # Free Memory  \n","      del model, outputs, X_data, y_data, true_labels, predicted_labels, precisions, accuracies, true_negatives, false_positives, precision\n","      del result_pred_labels\n","      torch.cuda.empty_cache()\n","      return [avg_precision, avg_accuracy, percent_positive]\n","  #-----------------------------------------------------------------------------------------------------------------------------------------------  \n","    start = 0\n","    curr = {}\n","    generator = prepare_train_data(train_data.iloc[500*2201:599*2201, :], 2201, seq_length=100, batch_size=1000)\n","  #----------------------------------------------------------------------------------------------------------------------------------------------------\n","    curr[\"train\"] = perform_validation1(num_models=1, file_desc=itr2, data_generator=generator)\n","    \n","    generator = prepare_train_data(test_data, 2201, seq_length=100, batch_size=1000)\n","  #----------------------------------------------------------------------------------------------------------------------------------------------------\n","    curr[\"val\"] = perform_validation1(num_models=1, file_desc=itr2, data_generator=generator)\n","    metrics.append(curr)\n","  return metrics\n","\n","metrics = perform_task_3()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"mpTd3kP-2lGX","executionInfo":{"status":"ok","timestamp":1683257767125,"user_tz":240,"elapsed":564275,"user":{"displayName":"Yash Oswal","userId":"08926553451935908326"}},"outputId":"a80aa80e-1dfa-4e76-d603-087e4bf27643"},"execution_count":34,"outputs":[{"output_type":"stream","name":"stderr","text":["<ipython-input-2-db319c480757>:44: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","  y_train_tensor = torch.tensor(labels_encoded, dtype=torch.float32).clone().detach()\n","<ipython-input-2-db319c480757>:44: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","  y_train_tensor = torch.tensor(labels_encoded, dtype=torch.float32).clone().detach()\n","<ipython-input-2-db319c480757>:44: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","  y_train_tensor = torch.tensor(labels_encoded, dtype=torch.float32).clone().detach()\n","<ipython-input-2-db319c480757>:44: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","  y_train_tensor = torch.tensor(labels_encoded, dtype=torch.float32).clone().detach()\n","<ipython-input-2-db319c480757>:44: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","  y_train_tensor = torch.tensor(labels_encoded, dtype=torch.float32).clone().detach()\n","<ipython-input-2-db319c480757>:44: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","  y_train_tensor = torch.tensor(labels_encoded, dtype=torch.float32).clone().detach()\n"]}]},{"cell_type":"code","source":["def print_table_task3(metrics):\n","    header = ['Model', 'Train', '', '', 'Validation', '', '', '', '']\n","    subheader = ['', '', 'Precision', 'Accuracy', '% Positive', 'Precision', 'Accuracy', '% Positive']\n","    print(\" {:<10} {:<10} {:<14} {:<10} {:<10} {:<14} {:<14} {:<14}\".format(*header))\n","    print(\"{:<10} {:<0} {:<10} {:<10} {:<14} {:<10} {:<10} {:<14}\".format(*subheader))\n","\n","    for i in range(len(metrics)):\n","        row =[i+1] + [f\"{val:.4f}\" for val in metrics[i]['train']] + [f\"{val:.4f}\" for val in metrics[i]['val']]\n","        print(\"{:<11} {:<10} {:<10} {:<14} {:<10} {:<10} {:<14}\".format(*row))\n","    return\n","print_table_task3(metrics)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ZcbXkhgj3NNp","executionInfo":{"status":"ok","timestamp":1683257870721,"user_tz":240,"elapsed":349,"user":{"displayName":"Yash Oswal","userId":"08926553451935908326"}},"outputId":"fa0f148e-27b9-40df-ce99-727ea344b8d9"},"execution_count":39,"outputs":[{"output_type":"stream","name":"stdout","text":[" Model      Train                                Validation                                             \n","            Precision  Accuracy   % Positive     Precision  Accuracy   % Positive    \n","1           0.3991     0.1421     0.3694         0.4851     0.1936     0.3154        \n","2           0.3744     0.2222     0.0050         0.4613     0.0632     0.0140        \n","3           0.5268     0.2455     0.0140         0.3223     0.0521     0.0020        \n","4           0.6488     0.2458     0.0000         0.2485     0.0509     0.0000        \n","5           0.5892     0.2456     0.0010         0.3069     0.0511     0.0000        \n","6           0.8606     0.2458     0.0000         0.2485     0.0509     0.0000        \n","7           0.8606     0.2458     0.0000         0.2485     0.0509     0.0000        \n","8           0.8606     0.2458     0.0000         0.2484     0.0509     0.0000        \n","9           0.5507     0.2455     0.0000         0.2484     0.0509     0.0000        \n","10          0.5036     0.2024     0.4505         0.3260     0.1972     0.3892        \n"]}]},{"cell_type":"markdown","source":["Task 3 Start"],"metadata":{"id":"G6a0oBa82SOX"}},{"cell_type":"markdown","source":[],"metadata":{"id":"LzGPGScN2UYI"}},{"cell_type":"markdown","source":["TASK 3 End"],"metadata":{"id":"sVQ-kdO-2QG_"}},{"cell_type":"markdown","source":["LSTM ARCHITECTURE\n","\n","---\n","\n"],"metadata":{"id":"s64CC3reenFc"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"Inw1vSq6mTI7"},"outputs":[],"source":["import torch\n","import torch.nn as nn\n","import torch.nn.init as init\n","from torch.utils.data import DataLoader, TensorDataset\n","from sklearn.metrics import accuracy_score, precision_score\n","from tqdm import tqdm\n","\n","class LSTM(nn.Module):\n","    def __init__(self, input_dim, hidden_dim, output_dim, isBatchNorm):\n","        super(LSTM, self).__init__()\n","        self.isBatchNorm = isBatchNorm\n","        self.hidden_dim = hidden_dim\n","        self.lstm = nn.LSTM(input_dim, hidden_dim, num_layers=1, batch_first=True) #num_layers = 2\n","        self.dropout = nn.Dropout(p=0.2)\n","        self.fc = nn.Linear(hidden_dim, output_dim)\n","        if self.isBatchNorm:\n","          self.bn = nn.BatchNorm1d(num_features=30) #Added New\n","        self.relu = nn.ReLU()\n","        self.softmax = nn.Softmax(dim=1)\n","        \n","        # Use Xavier initialization for weights\n","        init.xavier_uniform_(self.lstm.weight_ih_l0)\n","        init.orthogonal_(self.lstm.weight_hh_l0)\n","        init.constant_(self.lstm.bias_ih_l0, 0.0)\n","        init.constant_(self.lstm.bias_hh_l0, 0.0)\n","        init.xavier_uniform_(self.fc.weight)\n","        init.constant_(self.fc.bias, 0.0)\n","\n","    def forward(self, x):\n","        h0 = torch.zeros(1, x.size(0), self.hidden_dim).requires_grad_().to(device)\n","        c0 = torch.zeros(1, x.size(0), self.hidden_dim).requires_grad_().to(device)\n","        out, (hn, cn) = self.lstm(x, (h0.detach(), c0.detach()))\n","        if self.isBatchNorm:\n","          out = self.bn(out) #Added New\n","        out = self.dropout(out)\n","        out = self.fc(out[:, -1, :])\n","        out = self.relu(out)\n","        out = self.softmax(out)\n","        out = out.view(-1, 3)\n","        return out\n","\n","# Check if CUDA is available\n","device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"]},{"cell_type":"markdown","source":["Helper Function to train the model architecture\n","\n","---\n","\n"],"metadata":{"id":"8ODu8XHwehas"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"qgqXau2NQJ9Q"},"outputs":[],"source":["def train_model(model_id, generator, param_dict):\n","  # Evaluate each model for 10 epochs\n","  \n","  # Initialize the model\n","  learning_rate = param_dict[\"learning_rate\"]\n","  num_epochs = param_dict[\"num_epochs\"]\n","  input_dim = param_dict[\"input_dim\"]\n","  hidden_dim = param_dict[\"hidden_dim\"] #64\n","  output_dim = 3\n","  optimizer_name = param_dict[\"optimizer_name\"]\n","  isBatchNorm = param_dict[\"isBatchNorm\"]\n","  \n","  # Define the loss function and optimizer\n","  criterion = nn.CrossEntropyLoss()  \n","\n","  # Initialize the model\n","  print('Model no', model_id)\n","  model = LSTM(input_dim, hidden_dim, output_dim, isBatchNorm).to(device)\n","\n","  # Define the optimizer and scheduler for the current model\n","  # Define the optimizer and scheduler for the current model\n","  if optimizer_name == \"Adam\":\n","      optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n","  elif optimizer_name == \"SGD\":\n","      optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate, momentum=0.9)\n","  else:\n","      raise ValueError(f\"Unsupported optimizer {optimizer_name}\")\n","  scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=5, gamma=0.1) #Step Size for learning rate.\n","\n","  # Checkpointing parameters for the current model\n","  checkpoint_interval = 1\n","  checkpoint_file = f'lstm_checkpoint_{model_id}.pth'\n","\n","  # Define early stopping parameters\n","  best_loss = float('inf')\n","  early_stop_counter = 0\n","  early_stop_patience = 3\n","\n","  # Train the current model for 10 epochs\n","  for epoch in range(num_epochs):\n","    # Set model to train mode\n","    model.train()\n","    # model.double()\n","\n","    # Iterate over batches\n","    for x_batch, y_batch in tqdm(generator):\n","        # Move data to device\n","        x_batch, y_batch = x_batch.to(device), y_batch.to(device)\n","        \n","        # Cast x_batch to float32        \n","        x_batch = x_batch.float()\n","        \n","        # Forward pass\n","        outputs = model(x_batch)\n","        loss = criterion(outputs, y_batch)\n","\n","        # Backward and optimize\n","        optimizer.zero_grad()\n","        loss.backward()\n","        optimizer.step()\n","\n","    # Print the loss for every 10 epochs\n","    if (epoch + 1) % 10 == 0:\n","        print('Epoch [{}/{}], Loss: {:.4f}'.format(epoch + 1, num_epochs, loss.item()))\n","\n","    # Check if current model has reached early stopping criteria    \n","    if loss.item() < best_loss:\n","        best_loss = loss.item()\n","        early_stop_counter = 0\n","    else:\n","        early_stop_counter += 1\n","\n","    if early_stop_counter >= early_stop_patience:\n","        print(f\"Training for model {model_id} stopped early at epoch {epoch+1} due to early stopping\")\n","        break\n","\n","    # Save checkpoint at regular intervals\n","    if (epoch + 1) % checkpoint_interval == 0:\n","        torch.save({\n","            'epoch': epoch + 1,\n","            'model_state_dict': model.state_dict(),\n","            'optimizer_state_dict': optimizer.state_dict(),\n","            'loss': loss.item(),\n","            # 'bn_state_dict': model.bn.state_dict()\n","        }, checkpoint_file)\n","\n","    # Step the scheduler\n","    scheduler.step()\n","\n","  # Delete variables to free up memory\n","  del model, optimizer, scheduler\n","\n","  # Empty the cache to free up GPU memory\n","  torch.cuda.empty_cache()\n"]},{"cell_type":"markdown","source":["Function performs two steps\n","1. Incremental PCA\n","2. SelectKBest\n","---\n","\n","\n","\n"],"metadata":{"id":"PuwmEEMGeALm"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"5Bw7PXNdRwfx"},"outputs":[],"source":["from sklearn.model_selection import train_test_split\n","from sklearn.decomposition import IncrementalPCA\n","from sklearn.feature_selection import SelectKBest, f_regression\n","\n","def perform_PCA(data = None, topK = 10, topPCA=20):\n","  # Separate the target variable (stock price) from the features\n","  y = data['Label']\n","  X = data.drop(['Label'], axis=1)\n","\n","  # Split the dataset into training and testing sets\n","  X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, shuffle=False)\n","\n","  # Instantiate IncrementalPCA and fit to the training data in chunks\n","  batch_size = 1000\n","  \n","  ipca = IncrementalPCA(n_components=topPCA, batch_size=batch_size)\n","  for i in range(0, len(X_train), batch_size):\n","    X_train_batch = X_train.iloc[i:i+batch_size]\n","    ipca.partial_fit(X_train_batch)\n","\n","  # Transform the training and testing data in chunks\n","  X_train_pca = []\n","  for i in range(0, len(X_train), batch_size):\n","    X_train_batch = X_train.iloc[i:i+batch_size]\n","    X_train_pca_batch = ipca.transform(X_train_batch)\n","    X_train_pca.append(X_train_pca_batch)\n","  X_train_pca = np.vstack(X_train_pca)\n","\n","  X_test_pca = []\n","  for i in range(0, len(X_test), batch_size):\n","    X_test_batch = X_test.iloc[i:i+batch_size]\n","    X_test_pca_batch = ipca.transform(X_test_batch)\n","    X_test_pca.append(X_test_pca_batch)\n","  X_test_pca = np.vstack(X_test_pca)\n","  \n","  # Instantiate SelectKBest and fit to the training data\n","  selector = SelectKBest(f_regression, k=topK)  \n","  selector.fit(X_train_pca, y_train.cat.codes.astype('int').to_numpy())\n","\n","  # Transform the training and testing data\n","  X_train_selected = selector.transform(X_train_pca)\n","  X_test_selected = selector.transform(X_test_pca)\n","\n","  selected_indices = selector.get_support(indices=True)\n","\n","  # Select only the columns with the selected indices\n","  X_train_pca_df = pd.DataFrame(data=X_train_selected, columns=X_train.columns[selected_indices])\n","  X_test_pca_df = pd.DataFrame(data=X_test_selected, columns=X_train.columns[selected_indices])\n","\n","  # Assign labels to X_train_pca_df\n","  X_train_pca_df['Label'] = y_train.values\n","  X_test_pca_df['Label'] = y_test.values\n","  \n","  del X_train_selected, X_train_pca, X_train_batch\n","  del X_test_selected, X_test_pca, X_test_batch\n","  \n","  return X_train_pca_df, X_test_pca_df"]},{"cell_type":"markdown","source":["Helper function to calculate the 3 metrics\n","1. Preciosn\n","2. Accuracy\n","3. Percentage of Positive Prediction\n","\n","---\n","\n"],"metadata":{"id":"EBUYjoBhd1Te"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"3c4ytBwpdyuv"},"outputs":[],"source":["def perform_validation(num_models=1, file_desc=None, data_generator=None, param_dict=None):\n","  # Evaluate each model on the primary training set and the validation set  \n","  checkpoint_file = f'lstm_checkpoint_{file_desc}.pth' # Load the trained model from the checkpoint file\n","  checkpoint = torch.load(checkpoint_file, map_location=torch.device('cpu'))  \n","  model = LSTM(param_dict[\"input_dim\"], param_dict[\"hidden_dim\"], 3, param_dict[\"isBatchNorm\"]).to(device)\n","  model.load_state_dict(checkpoint['model_state_dict'])\n","  isValidation = param_dict[\"isValidation\"]\n","  isSkipResult = param_dict[\"isSkipResult\"]\n","\n","  # Evaluate the current model on the passed data  \n","  result_pred_labels = []  \n","  precisions = []\n","  accuracies = []\n","  for X_data, y_data in data_generator:        \n","    with torch.no_grad():\n","      X_data = X_data.float()\n","      outputs = model.forward(X_data.to(device))\n","\n","    # Convert predicted and true labels to one-hot encodings\n","    predicted_labels = torch.argmax(outputs, dim=1).to(device)\n","    if not isSkipResult:\n","      result_pred_labels.extend(predicted_labels)\n","    true_labels = torch.argmax(y_data, dim=1).to(device)\n","\n","    # Calculate precision for each class    \n","    for i in range(3):\n","      if i == 2: # positive class\n","          true_positives = torch.sum((predicted_labels == i) & (true_labels == i))\n","          false_positives = torch.sum((predicted_labels == i) & (true_labels != 2))\n","          precision = true_positives.float() / (true_positives + false_positives).float()\n","          accuracies.append(torch.mean((predicted_labels == true_labels).float() * (true_labels == i).float()))\n","      else: # negative class\n","          true_negatives = torch.sum((predicted_labels == i) & (true_labels == i) & (true_labels != 2))\n","          false_positives = torch.sum((predicted_labels == i) & (true_labels != i) & (true_labels != 2))\n","          precision = true_negatives.float() / (true_negatives + false_positives).float()\n","          accuracies.append(torch.mean((predicted_labels == true_labels).float() * (true_labels != i).float()))\n","      precisions.append(precision)\n","\n","  # Calculate average precision and accuracy\n","  avg_precision = torch.mean(torch.tensor(precisions))\n","  avg_accuracy = torch.mean(torch.tensor(accuracies))\n","\n","  # Calculate percentage of positive predictions\n","  percent_positive = torch.mean((predicted_labels == 2).float())\n","  \n","  if not isSkipResult:\n","    if isValidation:\n","      predictions[1].append(result_pred_labels)\n","    else:\n","      predictions[0].append(result_pred_labels)\n","  \n","  # Free Memory  \n","  del model, outputs, X_data, y_data, true_labels, predicted_labels, precisions, accuracies, true_negatives, false_positives, precision\n","  del result_pred_labels\n","  torch.cuda.empty_cache()\n","  return [avg_precision, avg_accuracy, percent_positive]"]},{"cell_type":"markdown","source":["Perform Voting"],"metadata":{"id":"rrjjwMj3dxtl"}},{"cell_type":"code","source":["def perform_voting(final_predictions, batch_generator):\n","  # Voting Technique\n","  cnt = 0\n","  precisions = []\n","  accuracies = []\n","  for X_batch, y_batch in batch_generator:    \n","    pred = final_predictions[cnt:cnt+len(y_batch)]\n","    true_labels = torch.argmax(y_batch, dim=1).to(device)\n","    for i in range(3):\n","      if i == 2: # positive class\n","          true_positives = torch.sum((pred == i) & (true_labels == i))\n","          false_positives = torch.sum((pred == i) & (true_labels != 2))\n","          precision = true_positives.float() / (true_positives + false_positives).float()\n","          accuracies.append(torch.mean((pred == true_labels).float() * (true_labels == i).float()))\n","      else: # negative class\n","          true_negatives = torch.sum((pred == i) & (true_labels == i) & (true_labels != 2))\n","          false_positives = torch.sum((pred == i) & (true_labels != i) & (true_labels != 2))\n","          precision = true_negatives.float() / (true_negatives + false_positives).float()\n","          accuracies.append(torch.mean((pred == true_labels).float() * (true_labels != i).float()))\n","      precisions.append(precision)\n","    \n","    cnt += len(y_batch)\n","\n","  # Calculate average precision and accuracy\n","  avg_precision = torch.mean(torch.tensor(precisions))\n","  avg_accuracy = torch.mean(torch.tensor(accuracies))\n","\n","  # Calculate percentage of positive predictions\n","  percent_positive = torch.mean((pred == 2).float())\n","  del X_batch, y_batch, true_labels, precisions, accuracies, true_negatives, false_positives, precision, pred\n","  torch.cuda.empty_cache()\n","  return [avg_precision, avg_accuracy, percent_positive]  "],"metadata":{"id":"rg1xPkXzK676"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Helper Function to Print Metric Table"],"metadata":{"id":"TeYLUXWvdrcU"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"xlLuGlUrHecD"},"outputs":[],"source":["def print_table(metrics):\n","    header = ['Model', 'Train', '', '', 'Validation', '', '', '', '']\n","    subheader = ['', '', 'Precision', 'Accuracy', '% Positive', 'Precision', 'Accuracy', '% Positive']\n","    print(\" {:<10} {:<10} {:<14} {:<10} {:<10} {:<14} {:<14} {:<14}\".format(*header))\n","    print(\"{:<10} {:<0} {:<10} {:<10} {:<14} {:<10} {:<10} {:<14}\".format(*subheader))\n","\n","    for i in range(len(metrics)):\n","        row =[i+1] + [f\"{val:.4f}\" for val in metrics[i]['train']] + [f\"{val:.4f}\" for val in metrics[i]['val']]\n","        print(\"{:<11} {:<10} {:<10} {:<14} {:<10} {:<10} {:<14}\".format(*row))\n","    return"]},{"cell_type":"code","source":["def print_test_table(metrics):\n","    header = ['Model', 'Validation', '', '', '', '']\n","    subheader = ['', '', 'Precision', 'Accuracy', '% Positive']\n","    print(\" {:<10} {:<10} {:<14} {:<14} {:<14}\".format(*header))\n","    print(\"{:<10} {:<0} {:<10} {:<10} {:<14}\".format(*subheader))\n","\n","    for i in range(len(metrics)):\n","        row =[i+1]  + [f\"{val:.4f}\" for val in metrics[i]['val']]\n","        print(\"{:<11} {:<10} {:<10} {:<14}\".format(*row))\n","    return"],"metadata":{"id":"jV2mlqDdWmCF"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["All the required functions are defined above these cell."],"metadata":{"id":"Z6yd4qJUbyid"}},{"cell_type":"markdown","source":["Task 4 Start"],"metadata":{"id":"asDEU5Wq9F1x"}},{"cell_type":"code","source":["#Perform PCA and Feature Selection to get top n features from m selected Principal Components (Upto 10 models trained).\n","X_train_pca_df, X_test_pca_df = perform_PCA(train_data[:2201*800], 10, 20) \n","\n","#Train atleast 10 models on the PCA Transformed data.\n","seq_length = 30 # Define sequence length\n","batch_size = stock_len # Create data loader\n","\n","param_dict = {\n","    \"input_dim\": 10,\n","    \"hidden_dim\": 32,\n","    \"learning_rate\": 0.0001,\n","    \"num_epochs\": 20,\n","    \"optimizer_name\": 'Adam',\n","    \"isBatchNorm\": False    \n","}\n","\n","cnt = 0\n","while cnt < 10:\n","  #Generator function to load data in batches/chunks and get max performance\n","  generator = prepare_train_data(X_train_pca_df, 2201, seq_length, batch_size)\n","  train_model(f'{cnt}_pca_800_step4', generator, param_dict)\n","  del generator\n","  cnt += 1"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"FIoAX3Dfxxp6","executionInfo":{"status":"ok","timestamp":1683218946879,"user_tz":240,"elapsed":2520320,"user":{"displayName":"Pran En","userId":"14838666787094319667"}},"outputId":"0119cd05-66f2-47cb-f597-e2babaa298e9"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Model no 0_pca_800_step4\n"]},{"output_type":"stream","name":"stderr","text":["\r0it [00:00, ?it/s]<ipython-input-6-8fa599ece79a>:44: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","  y_train_tensor = torch.tensor(labels_encoded).float().clone().detach()\n","632it [04:06,  2.56it/s]\n","0it [00:00, ?it/s]\n","0it [00:00, ?it/s]\n","0it [00:00, ?it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Training for model 0_pca_800_step4 stopped early at epoch 4 due to early stopping\n","Model no 1_pca_800_step4\n"]},{"output_type":"stream","name":"stderr","text":["632it [03:57,  2.66it/s]\n","0it [00:00, ?it/s]\n","0it [00:00, ?it/s]\n","0it [00:00, ?it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Training for model 1_pca_800_step4 stopped early at epoch 4 due to early stopping\n","Model no 2_pca_800_step4\n"]},{"output_type":"stream","name":"stderr","text":["632it [04:16,  2.46it/s]\n","0it [00:00, ?it/s]\n","0it [00:00, ?it/s]\n","0it [00:00, ?it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Training for model 2_pca_800_step4 stopped early at epoch 4 due to early stopping\n","Model no 3_pca_800_step4\n"]},{"output_type":"stream","name":"stderr","text":["632it [04:37,  2.28it/s]\n","0it [00:00, ?it/s]\n","0it [00:00, ?it/s]\n","0it [00:00, ?it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Training for model 3_pca_800_step4 stopped early at epoch 4 due to early stopping\n","Model no 4_pca_800_step4\n"]},{"output_type":"stream","name":"stderr","text":["632it [03:50,  2.74it/s]\n","0it [00:00, ?it/s]\n","0it [00:00, ?it/s]\n","0it [00:00, ?it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Training for model 4_pca_800_step4 stopped early at epoch 4 due to early stopping\n","Model no 5_pca_800_step4\n"]},{"output_type":"stream","name":"stderr","text":["632it [04:04,  2.59it/s]\n","0it [00:00, ?it/s]\n","0it [00:00, ?it/s]\n","0it [00:00, ?it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Training for model 5_pca_800_step4 stopped early at epoch 4 due to early stopping\n","Model no 6_pca_800_step4\n"]},{"output_type":"stream","name":"stderr","text":["632it [03:55,  2.68it/s]\n","0it [00:00, ?it/s]\n","0it [00:00, ?it/s]\n","0it [00:00, ?it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Training for model 6_pca_800_step4 stopped early at epoch 4 due to early stopping\n","Model no 7_pca_800_step4\n"]},{"output_type":"stream","name":"stderr","text":["632it [04:12,  2.50it/s]\n","0it [00:00, ?it/s]\n","0it [00:00, ?it/s]\n","0it [00:00, ?it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Training for model 7_pca_800_step4 stopped early at epoch 4 due to early stopping\n","Model no 8_pca_800_step4\n"]},{"output_type":"stream","name":"stderr","text":["632it [03:56,  2.67it/s]\n","0it [00:00, ?it/s]\n","0it [00:00, ?it/s]\n","0it [00:00, ?it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Training for model 8_pca_800_step4 stopped early at epoch 4 due to early stopping\n","Model no 9_pca_800_step4\n"]},{"output_type":"stream","name":"stderr","text":["632it [04:13,  2.49it/s]\n","0it [00:00, ?it/s]\n","0it [00:00, ?it/s]\n","0it [00:00, ?it/s]"]},{"output_type":"stream","name":"stdout","text":["Training for model 9_pca_800_step4 stopped early at epoch 4 due to early stopping\n"]},{"output_type":"stream","name":"stderr","text":["\n"]}]},{"cell_type":"code","execution_count":null,"metadata":{"id":"9jmZ7sX-2NYd","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1683219960236,"user_tz":240,"elapsed":570222,"user":{"displayName":"Pran En","userId":"14838666787094319667"}},"outputId":"a2238f64-428d-46bb-ef50-437dcaa72d5a"},"outputs":[{"output_type":"stream","name":"stderr","text":["<ipython-input-6-8fa599ece79a>:44: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","  y_train_tensor = torch.tensor(labels_encoded).float().clone().detach()\n","<ipython-input-6-8fa599ece79a>:44: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","  y_train_tensor = torch.tensor(labels_encoded).float().clone().detach()\n","<ipython-input-6-8fa599ece79a>:44: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","  y_train_tensor = torch.tensor(labels_encoded).float().clone().detach()\n","<ipython-input-6-8fa599ece79a>:44: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","  y_train_tensor = torch.tensor(labels_encoded).float().clone().detach()\n","<ipython-input-6-8fa599ece79a>:44: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","  y_train_tensor = torch.tensor(labels_encoded).float().clone().detach()\n","<ipython-input-6-8fa599ece79a>:44: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","  y_train_tensor = torch.tensor(labels_encoded).float().clone().detach()\n","<ipython-input-6-8fa599ece79a>:44: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","  y_train_tensor = torch.tensor(labels_encoded).float().clone().detach()\n","<ipython-input-6-8fa599ece79a>:44: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","  y_train_tensor = torch.tensor(labels_encoded).float().clone().detach()\n","<ipython-input-6-8fa599ece79a>:44: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","  y_train_tensor = torch.tensor(labels_encoded).float().clone().detach()\n"]},{"output_type":"stream","name":"stdout","text":[" Model      Train                                Validation                                             \n","            Precision  Accuracy   % Positive     Precision  Accuracy   % Positive    \n","1           0.5628     0.2093     0.2305         0.4165     0.1700     0.1758        \n","2           0.5736     0.2018     0.1857         0.4398     0.1899     0.1093        \n","3           0.5867     0.2137     0.2537         0.4476     0.1933     0.1864        \n","4           0.5748     0.2207     0.2736         0.4451     0.1967     0.1686        \n","5           0.5654     0.1910     0.1360         0.4595     0.1791     0.0987        \n","6           0.5430     0.2157     0.2919         0.4328     0.1891     0.2141        \n","7           0.5520     0.2315     0.3118         0.4502     0.1904     0.1830        \n","8           0.5834     0.2091     0.1874         0.4239     0.1899     0.1326        \n","9           0.5549     0.2398     0.3599         0.4110     0.2017     0.2551        \n","10          0.5871     0.1955     0.1211         0.4272     0.1814     0.1087        \n"]}],"source":["# Concatenate tensors of empty(Provding early shape can provide performance improvement)\n","predictions = [[],[]]\n","metrics = []\n","param_dict = {\n","    \"input_dim\": 10,\n","    \"hidden_dim\": 32,\n","    \"learning_rate\": 0.0001,\n","    \"num_epochs\": 20,\n","    \"optimizer_name\": 'Adam',\n","    \"isBatchNorm\": False,\n","    \"isValidation\": False,\n","    \"isSkipResult\": False\n","}\n","\n","cnt = 0\n","while cnt < 10:\n","  curr = {}\n","  batch_generator = prepare_train_data(X_train_pca_df.iloc[:2201*200], 2201, 30, stock_len) #Using subset of training data due to ram issue but model trained on huge data.\n","  param_dict[\"isValidation\"] = False\n","  curr[\"train\"] = perform_validation(1, f'{cnt}_pca_800_step4', batch_generator, param_dict)\n","  batch_generator = prepare_train_data(X_test_pca_df, 2201, 30, stock_len)\n","  param_dict[\"isValidation\"] = True\n","  curr[\"val\"] = perform_validation(1, f'{cnt}_pca_800_step4', batch_generator, param_dict)\n","  metrics.append(curr)\n","  del batch_generator, curr\n","  cnt += 1\n","print_table(metrics)"]},{"cell_type":"code","source":["# Concatenate tensors of empty(Provding early shape can provide performance improvement)\n","predictions = [[],[]]\n","metrics = []\n","param_dict = {\n","    \"input_dim\": 10,\n","    \"hidden_dim\": 32,\n","    \"learning_rate\": 0.0001,\n","    \"num_epochs\": 20,\n","    \"optimizer_name\": 'Adam',\n","    \"isBatchNorm\": False,\n","    \"isValidation\": False,\n","    \"isSkipResult\": False\n","}\n","\n","cnt = 0\n","while cnt < 10:\n","  curr = {}\n","  batch_generator = prepare_train_data(X_train_pca_df.iloc[:2201*200], 2201, 30, stock_len) #Using subset of training data due to ram issue but model trained on huge data.\n","  param_dict[\"isValidation\"] = False\n","  curr[\"train\"] = perform_validation(1, f'{cnt}_pca_800', batch_generator, param_dict)\n","  batch_generator = prepare_train_data(X_test_pca_df, 2201, 30, stock_len)\n","  param_dict[\"isValidation\"] = True\n","  curr[\"val\"] = perform_validation(1, f'{cnt}_pca_800', batch_generator, param_dict)\n","  metrics.append(curr)\n","  del batch_generator, curr\n","  cnt += 1\n","print_table(metrics)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"bLfO3huNxlHZ","executionInfo":{"status":"ok","timestamp":1683221994768,"user_tz":240,"elapsed":491,"user":{"displayName":"Pran En","userId":"14838666787094319667"}},"outputId":"69a2076b-8fe8-4e1a-d3d3-3113904bd20a"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":[" Model      Train                                Validation                                             \n","            Precision  Accuracy   % Positive     Precision  Accuracy   % Positive    \n","1           0.5828     0.2209     0.2852         0.4429     0.1788     0.1320        \n","2           0.5894     0.2213     0.2852         0.4291     0.1925     0.1398        \n","3           0.5534     0.1900     0.1708         0.4150     0.1499     0.1120        \n","4           0.6004     0.2360     0.3068         0.4236     0.1973     0.2080        \n","5           0.5987     0.2190     0.2172         0.4439     0.1868     0.1375        \n","6           0.5907     0.2103     0.2156         0.4269     0.1790     0.1337        \n","7           0.5778     0.2157     0.2388         0.4240     0.1807     0.1608        \n","8           0.5544     0.2123     0.2272         0.4101     0.1682     0.2546        \n","9           0.6055     0.2148     0.1990         0.4450     0.1870     0.1021        \n","10          0.5791     0.2237     0.2786         0.4283     0.1743     0.1681        \n"]}]},{"cell_type":"markdown","source":["Task 4 End"],"metadata":{"id":"Wo2G8Zz89I4B"}},{"cell_type":"markdown","source":["Task 5 Start"],"metadata":{"id":"iEd0bwa19KZh"}},{"cell_type":"code","source":["#Voting\n","from collections import Counter\n","\n","curr = {}\n","for i in range(2):\n","  final_predictions = [Counter(p).most_common(1)[0][0] for p in zip(*predictions[i])]\n","  final_predictions = torch.stack(final_predictions)\n","  if i == 0:\n","    batch_generator = prepare_train_data(X_train_pca_df.iloc[:2201*200], 2201, 30, stock_len)\n","    curr[\"train\"] = perform_voting(final_predictions, batch_generator)\n","  elif i == 1:\n","    batch_generator = prepare_train_data(X_test_pca_df, 2201, 30, stock_len)\n","    curr[\"val\"] = perform_voting(final_predictions, batch_generator)\n","print_table([curr])\n","del final_predictions, batch_generator, curr"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"CZnzY_00xRhY","executionInfo":{"status":"ok","timestamp":1683222143513,"user_tz":240,"elapsed":22464,"user":{"displayName":"Pran En","userId":"14838666787094319667"}},"outputId":"ea52d5fe-088d-44fa-fbe7-2e63c49a4f66"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["<ipython-input-4-8fa599ece79a>:44: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","  y_train_tensor = torch.tensor(labels_encoded).float().clone().detach()\n"]},{"output_type":"stream","name":"stdout","text":[" Model      Train                                Validation                                             \n","            Precision  Accuracy   % Positive     Precision  Accuracy   % Positive    \n","1           0.5828     0.2209     0.2852         0.4429     0.1788     0.1320        \n"]}]},{"cell_type":"code","execution_count":null,"metadata":{"id":"QEwMJRfhj22H","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1683220370407,"user_tz":240,"elapsed":30451,"user":{"displayName":"Pran En","userId":"14838666787094319667"}},"outputId":"9772b813-c6e7-4126-d0e1-40936dec64be"},"outputs":[{"output_type":"stream","name":"stderr","text":["<ipython-input-6-8fa599ece79a>:44: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","  y_train_tensor = torch.tensor(labels_encoded).float().clone().detach()\n"]},{"output_type":"stream","name":"stdout","text":[" Model      Train                                Validation                                             \n","            Precision  Accuracy   % Positive     Precision  Accuracy   % Positive    \n","1           0.5628     0.2093     0.2305         0.4165     0.1700     0.1758        \n"]}],"source":["#Voting 2\n","from collections import Counter\n","\n","curr = {}\n","for i in range(2):\n","  final_predictions = [Counter(p).most_common(1)[0][0] for p in zip(*predictions[i])]\n","  final_predictions = torch.stack(final_predictions)\n","  if i == 0:\n","    batch_generator = prepare_train_data(X_train_pca_df.iloc[:2201*200], 2201, 30, stock_len)\n","    curr[\"train\"] = perform_voting(final_predictions, batch_generator)\n","  elif i == 1:\n","    batch_generator = prepare_train_data(X_test_pca_df, 2201, 30, stock_len)\n","    curr[\"val\"] = perform_voting(final_predictions, batch_generator)\n","print_table([curr])\n","del final_predictions, batch_generator, curr"]},{"cell_type":"markdown","source":["Task 5 End"],"metadata":{"id":"krLLDNV49duT"}},{"cell_type":"markdown","source":["Validaiton Set Testing on Best Model"],"metadata":{"id":"jAzTZOCJ9h-C"}},{"cell_type":"code","source":["def perform_test_on_validation_set():\n","  \n","  #Load Feature Engineered Stock dataset: 100 Features including labels\n","  with open('data_with_features_clean_test.pkl', 'rb') as test_file: #To do: Change file name.\n","    test_data = pickle.load(test_file)\n","  del test_file  \n","\n","  #Perform PCA and Feature Selection to get top n features from m selected Principal Components.\n","  X_train_pca_df, X_test_pca_df = perform_PCA(test_data, 10, 20)\n","  dataset = pd.concat([X_train_pca_df, X_test_pca_df])\n","  del X_train_pca_df, X_test_pca_df, test_data\n","\n","  #Call Evaluation\n","  predictions = [[],[]]\n","  metrics = []\n","  param_dict = {\n","      \"input_dim\": 10,\n","      \"hidden_dim\": 32,\n","      \"learning_rate\": 0.0001,\n","      \"num_epochs\": 10,\n","      \"optimizer_name\": 'Adam',\n","      \"isBatchNorm\": False,\n","      \"isValidation\": True,\n","      \"isSkipResult\": True\n","  }\n","  batch_generator = prepare_train_data(dataset, 2201, 30, stock_len)\n","  curr = {}\n","  curr[\"val\"] = perform_validation(1, '1_pca_800', batch_generator, param_dict)\n","  \n","  return curr\n","\n","curr = perform_test_on_validation_set()"],"metadata":{"id":"6cGXLxmZSdwR"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["print_test_table([curr])"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"LpA8aZz26vXp","executionInfo":{"status":"ok","timestamp":1683223174577,"user_tz":240,"elapsed":446,"user":{"displayName":"Pran En","userId":"14838666787094319667"}},"outputId":"56a6d81b-954e-42f6-f3b7-85e75d7c66f7"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":[" Model      Validation                                             \n","            Precision  Accuracy   % Positive    \n","1           0.4554     0.1783     0.3724        \n"]}]},{"cell_type":"markdown","source":["Validation Set testing on Best Model End"],"metadata":{"id":"VA3uwNBoYc0n"}},{"cell_type":"markdown","source":["Testing Set 1\n"],"metadata":{"id":"YIGLt6J-cEIk"}},{"cell_type":"code","source":["def perform_test_on_testing_set1():\n","  \n","  # Load Feature Engineered Stock dataset: 100 Features including labels\n","  with open('testing_set1_processed.pkl', 'rb') as test_file: #Feature Engineered File\n","    test_data = pickle.load(test_file)\n","  del test_file\n","\n","  # Perform PCA and Feature Selection to get top n features| from m selected Principal Components.\n","  X_train_pca_df, X_test_pca_df = perform_PCA(test_data, 10, 20)\n","  dataset = pd.concat([X_train_pca_df, X_test_pca_df])\n","  del X_train_pca_df, X_test_pca_df, test_data\n","\n","  # Call Evaluation\n","  predictions = [[],[]]\n","  metrics = []\n","\n","  param_dict = {\n","      \"input_dim\": 10,\n","      \"hidden_dim\": 32,\n","      \"learning_rate\": 0.0001,\n","      \"num_epochs\": 10,\n","      \"optimizer_name\": 'Adam',\n","      \"isBatchNorm\": False,\n","      \"isValidation\": True,\n","      \"isSkipResult\": True\n","  }\n","  batch_generator = prepare_train_data(dataset, 888, 30, 1000)\n","  curr = {}  \n","  curr[\"val\"] = perform_validation(1, '1_pca_800', batch_generator, param_dict)\n","  return curr\n","\n","curr = perform_test_on_testing_set1()"],"metadata":{"id":"nG7OWpGaZ-xX"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["print_test_table([curr])"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"lGozzK13-hA_","executionInfo":{"status":"ok","timestamp":1683224416392,"user_tz":240,"elapsed":497,"user":{"displayName":"Yash Oswal","userId":"08926553451935908326"}},"outputId":"116b1170-f690-427c-d500-b1628a0c61a3"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":[" Model      Validation                                             \n","            Precision  Accuracy   % Positive    \n","1           0.4247     0.1587     0.2414        \n"]}]},{"cell_type":"markdown","source":["Testing Set 1 End"],"metadata":{"id":"aZlkmhwZ97H6"}},{"cell_type":"markdown","source":["Testing Set 2 Start"],"metadata":{"id":"SZd-U0VNe4QN"}},{"cell_type":"code","source":["def perform_test_on_testing_set2():\n","  \n","  # Load Feature Engineered Stock dataset: 100 Features including labels\n","  with open('testing_set2_processed.pkl', 'rb') as test_file: #Feature Engineered File\n","    test_data = pickle.load(test_file)\n","  del test_file\n","  \n","  # Perform PCA and Feature Selection to get top n features| from m selected Principal Components.\n","  X_train_pca_df, X_test_pca_df = perform_PCA(test_data, 10, 20)\n","  dataset = pd.concat([X_train_pca_df, X_test_pca_df])\n","  del X_train_pca_df, X_test_pca_df, test_data\n","\n","  # # Call Evaluation\n","  predictions = [[],[]]\n","  metrics = []\n","\n","  param_dict = {\n","      \"input_dim\": 10,\n","      \"hidden_dim\": 32,\n","      \"learning_rate\": 0.0001,\n","      \"num_epochs\": 10,\n","      \"optimizer_name\": 'Adam',\n","      \"isBatchNorm\": False,\n","      \"isValidation\": True,\n","      \"isSkipResult\": True\n","  }\n","  batch_generator = prepare_train_data(dataset, 705, 30, 1000)\n","  curr = {}  \n","  curr[\"val\"] = perform_validation(1, '1_pca_800', batch_generator, param_dict)\n","  return curr\n","\n","curr = perform_test_on_testing_set2()"],"metadata":{"id":"ca84vL6ne3Pd"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["print_test_table([curr])"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"6Qw793QwfCMz","executionInfo":{"status":"ok","timestamp":1683233509216,"user_tz":240,"elapsed":16,"user":{"displayName":"Yash Oswal","userId":"08926553451935908326"}},"outputId":"5679a91d-2d09-4cea-d7a3-0f6654ca76f8"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":[" Model      Validation                                             \n","            Precision  Accuracy   % Positive    \n","1           0.4306     0.2123     0.2489        \n"]}]},{"cell_type":"markdown","source":["Testing Set 2 End"],"metadata":{"id":"z_jJx-6ye5_u"}},{"cell_type":"markdown","source":["Testing Set 3 Start"],"metadata":{"id":"uuRF12Wdv634"}},{"cell_type":"code","source":["def perform_test_on_testing_set3():\n","\n","  # Load Feature Engineered Stock dataset: 100 Features including labels\n","  with open('testing_set3_processed.pkl', 'rb') as test_file: #Feature Engineered File\n","    test_data = pickle.load(test_file)\n","  del test_file\n","\n","  # Perform PCA and Feature Selection to get top n features| from m selected Principal Components.\n","  X_train_pca_df, X_test_pca_df = perform_PCA(test_data, 10, 20)\n","  dataset = pd.concat([X_train_pca_df, X_test_pca_df])\n","  del X_train_pca_df, X_test_pca_df, test_data\n","\n","  # # Call Evaluation\n","  predictions = [[],[]]\n","  metrics = []\n","\n","  param_dict = {\n","      \"input_dim\": 10,\n","      \"hidden_dim\": 32,\n","      \"learning_rate\": 0.0001,\n","      \"num_epochs\": 10,\n","      \"optimizer_name\": 'Adam',\n","      \"isBatchNorm\": False,\n","      \"isValidation\": True,\n","      \"isSkipResult\": True\n","  }\n","  batch_generator = prepare_train_data(dataset, 4598, 30, 1000)\n","  curr = {}  \n","  curr[\"val\"] = perform_validation(1, '1_pca_800', batch_generator, param_dict)\n","  return curr\n","\n","curr = perform_test_on_testing_set3()"],"metadata":{"id":"w2FCU1jCv6Xw"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["print_test_table([curr])"],"metadata":{"id":"DyGfYW36wBkD","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1683248348991,"user_tz":240,"elapsed":7,"user":{"displayName":"Yash Oswal","userId":"08926553451935908326"}},"outputId":"96f4a590-1e8b-45d0-9fb7-8e42457a4bfa"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":[" Model      Validation                                             \n","            Precision  Accuracy   % Positive    \n","1           0.4517     0.1736     0.2693        \n"]}]},{"cell_type":"markdown","source":["Testing Set 3 End"],"metadata":{"id":"60h1SXVov9JR"}},{"cell_type":"code","source":["import torch\n","import torch.nn as nn\n","import torch.nn.init as init\n","from torch.utils.data import DataLoader, TensorDataset\n","from sklearn.metrics import accuracy_score, precision_score\n","from tqdm import tqdm\n","\n","class LSTM(nn.Module):\n","    def __init__(self, input_dim, hidden_dim, output_dim, isBatchNorm):\n","        super(LSTM, self).__init__()\n","        self.isBatchNorm = isBatchNorm\n","        self.hidden_dim = hidden_dim\n","        self.lstm1 = nn.LSTM(input_dim, hidden_dim, num_layers=2, batch_first=True)\n","        self.lstm2 = nn.LSTM(hidden_dim, hidden_dim, num_layers=2, batch_first=True)\n","        self.dropout = nn.Dropout(p=0.2)\n","        self.fc1 = nn.Linear(hidden_dim, 128)\n","        self.fc2 = nn.Linear(128, output_dim)  # Change output_dim to 3 for 3 classes\n","        if self.isBatchNorm:\n","            self.bn = nn.BatchNorm1d(num_features=30)\n","        self.relu = nn.ReLU()\n","        self.softmax = nn.Softmax(dim=1)\n","        \n","        # Use Xavier initialization for weights\n","        init.xavier_uniform_(self.lstm1.weight_ih_l0)\n","        init.orthogonal_(self.lstm1.weight_hh_l0)\n","        init.constant_(self.lstm1.bias_ih_l0, 0.0)\n","        init.constant_(self.lstm1.bias_hh_l0, 0.0)\n","        init.xavier_uniform_(self.lstm2.weight_ih_l0)\n","        init.orthogonal_(self.lstm2.weight_hh_l0)\n","        init.constant_(self.lstm2.bias_ih_l0, 0.0)\n","        init.constant_(self.lstm2.bias_hh_l0, 0.0)\n","        init.xavier_uniform_(self.fc1.weight)\n","        init.constant_(self.fc1.bias, 0.0)\n","        init.xavier_uniform_(self.fc2.weight)\n","        init.constant_(self.fc2.bias, 0.0)\n","\n","\n","\n","    def forward(self, x):\n","        h01 = torch.zeros(2, x.size(0), self.hidden_dim).requires_grad_().to(device)\n","        c01 = torch.zeros(2, x.size(0), self.hidden_dim).requires_grad_().to(device)\n","        out1, (hn1, cn1) = self.lstm1(x, (h01.detach(), c01.detach()))\n","        h02 = torch.zeros(2, x.size(0), self.hidden_dim).requires_grad_().to(device)\n","        c02 = torch.zeros(2, x.size(0), self.hidden_dim).requires_grad_().to(device)\n","        out2, (hn2, cn2) = self.lstm2(out1, (h02.detach(), c02.detach()))\n","        if self.isBatchNorm:\n","            out2 = self.bn(out2)\n","        out2 = self.dropout(out2)\n","        out2 = self.fc1(out2[:, -1, :])\n","        out2 = self.relu(out2)\n","        out2 = self.fc2(out2)  # remove [:, -1]\n","        out2 = self.softmax(out2)\n","        out2 = out2.view(-1, 3)\n","\n","        # out2 = self.fc1(out2[:, -1, :])\n","        # out2 = self.relu(out2)\n","        # out2 = self.fc2(out2[:, -1])\n","        # out2 = self.softmax(out2)\n","        # out2 = out2.view(-1, 3)\n","        return out2\n","\n","# Check if CUDA is available\n","device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"],"metadata":{"id":"RvnWoWgXpW__"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#Perform PCA and Feature Selection to get top n features from m selected Principal Components (Upto 10 models trained).\n","\n","X_train_pca_df, X_test_pca_df = perform_PCA(train_data, 20, 50)\n","dataset = pd.concat([X_train_pca_df,X_test_pca_df])\n","seq_length = 30 # Define sequence length\n","stock_len = 2201\n","batch_size = stock_len # Create data loader\n","\n","param_dict = {\n","    \"input_dim\": 20,\n","    \"hidden_dim\": 64,\n","    \"learning_rate\": 0.001,\n","    \"num_epochs\": 30,    \n","    \"optimizer_name\": 'SGD',\n","    \"isBatchNorm\": True,\n","}\n","\n","# param_dict = {\n","#     \"input_dim\": 10,\n","#     \"hidden_dim\": 64,\n","#     \"learning_rate\": 0.001,\n","#     \"num_epochs\": 30,\n","#     \"optimizer\": 'SGG',\n","#     \"isBatchNorm\": False,\n","# }\n","\n","# param_dict = {\n","#     \"input_dim\": 10,\n","#     \"hidden_dim\": 128,\n","#     \"learning_rate\": 0.0005,\n","#     \"num_epochs\": 25,\n","#     \"optimizer\": torch.optim.Adagrad,\n","#     \"isBatchNorm\": True,\n","# }\n","\n","\n","#Generator function to load data in batches/chunks and get max performance\n","generator = prepare_train_data(dataset, stock_len, seq_length, batch_size)\n","train_model(f'{2}_pca_best_model', generator, param_dict)\n","del generator\n"],"metadata":{"id":"U7zTs4yZSNbh","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1683240331042,"user_tz":240,"elapsed":123185,"user":{"displayName":"Yash Oswal","userId":"08926553451935908326"}},"outputId":"0e05e670-30ff-4f45-b3d8-6173d61eb295"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Model no 2_pca_best_model\n"]},{"output_type":"stream","name":"stderr","text":["0it [00:00, ?it/s]<ipython-input-53-db319c480757>:44: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","  y_train_tensor = torch.tensor(labels_encoded, dtype=torch.float32).clone().detach()\n","1578it [02:01, 13.03it/s]\n","0it [00:00, ?it/s]\n","0it [00:00, ?it/s]\n","0it [00:00, ?it/s]"]},{"output_type":"stream","name":"stdout","text":["Training for model 2_pca_best_model stopped early at epoch 4 due to early stopping\n"]},{"output_type":"stream","name":"stderr","text":["\n"]}]},{"cell_type":"code","source":["import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","import numpy as np\n","import pandas as pd\n","\n","# Define the environment\n","class TradingEnv:\n","    def __init__(self, data):\n","        self.data = data\n","        self.reset()\n","\n","    def reset(self):\n","        self.t = 0\n","        self.profit = 0\n","        self.bought = False\n","        self.stock_price = self.data.iloc[self.t, 0]\n","        self.cash = 10000\n","        self.shares = 0\n","\n","    def step(self, action):\n","        reward = 0\n","        done = False\n","\n","        if action == 0 and not self.bought:\n","            # Buy shares\n","            self.bought = True\n","            self.shares = self.cash / self.stock_price\n","            self.cash = 0\n","        elif action == 1 and self.bought:\n","            # Sell shares\n","            self.bought = False\n","            self.cash = self.stock_price * self.shares\n","            self.shares = 0\n","\n","            # Calculate profit\n","            self.profit += self.cash - 10000\n","            reward = self.profit\n","\n","        # Move to the next time step\n","        self.t += 1\n","        if self.t >= len(self.data):\n","            done = True\n","        else:\n","            self.stock_price = self.data.iloc[self.t, 0]\n","\n","        return reward, done\n","\n","class PolicyNetwork(nn.Module):\n","    def __init__(self, input_size, output_size):\n","        super().__init__()\n","        self.fc1 = nn.Linear(input_size, 128)\n","        self.fc2 = nn.Linear(128, 64)\n","        self.fc3 = nn.Linear(64, 32)\n","        self.fc4 = nn.Linear(32, output_size)\n","\n","    def forward(self, x):\n","        x = torch.relu(self.fc1(x))\n","        x = torch.relu(self.fc2(x))\n","        x = torch.relu(self.fc3(x))\n","        x = torch.softmax(self.fc4(x), dim=-1)\n","        return x\n","\n","\n","# Train the policy network\n","def train(env, agent, optimizer, device, num_episodes, batch_size, gamma):\n","    rewards = []\n","    for i_episode in range(num_episodes):\n","        state = env.reset()\n","        log_probs = []\n","        rewards_episode = []\n","        done = False\n","        while not done:\n","            state_tensor = torch.tensor(state, dtype=torch.float32).unsqueeze(0).to(device)\n","            action, log_prob = agent.get_action_and_log_prob(state_tensor)\n","            next_state, reward, done, _ = env.step(action)\n","            log_probs.append(log_prob)\n","            rewards_episode.append(reward)\n","            state = next_state\n","        rewards.append(sum(rewards_episode))\n","        \n","        # compute returns and advantages\n","        returns = compute_returns(rewards_episode, gamma)\n","        advantages = compute_advantages(rewards_episode, agent.value_function(state_tensor), gamma)\n","        \n","        # concatenate lists of log probabilities, states, returns, and advantages\n","        log_probs_tensor = torch.cat(log_probs)\n","        returns_tensor = torch.tensor(returns, dtype=torch.float32).unsqueeze(1).to(device)\n","        advantages_tensor = torch.tensor(advantages, dtype=torch.float32).unsqueeze(1).to(device)\n","        \n","        # update policy and value function\n","        loss = agent.update_policy_and_value_function(log_probs_tensor, state_tensor, returns_tensor, advantages_tensor, optimizer, batch_size)\n","    \n","    return rewards\n","\n"],"metadata":{"id":"LfV7xH9Ja4zq"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import torch.optim as optim\n","from torch.distributions.categorical import Categorical\n","\n","# Set up the trading environment\n","env = TradingEnv(train_data)\n","\n","# Define the neural network model\n","model = PolicyNetwork(101, 3)\n","\n","# Define the optimizer\n","optimizer = optim.Adam(model.parameters(), lr=0.001)\n","\n","# Set the number of episodes to run\n","num_episodes = 100\n","\n","# Train the model\n","for episode in range(num_episodes):\n","    state = env.get_state()\n","    done = False\n","    while not done:\n","        # Get the action probabilities from the model\n","        logits = model(torch.tensor(state, dtype=torch.float32))\n","        action_probs = torch.softmax(logits, dim=0)\n","\n","        # Sample an action from the action probabilities\n","        action_dist = Categorical(action_probs)\n","        action = action_dist.sample()\n","\n","        # Take a step in the environment\n","        reward, done = env.step(action.item())\n","\n","        # Update the state and compute the loss\n","        next_state = env.get_state()\n","        state_tensor = torch.tensor(state, dtype=torch.float32)\n","        action_tensor = torch.tensor(action.item(), dtype=torch.int64)\n","        reward_tensor = torch.tensor(reward, dtype=torch.float32)\n","        next_state_tensor = torch.tensor(next_state, dtype=torch.float32)\n","        done_tensor = torch.tensor(done, dtype=torch.float32)\n","        log_prob = action_dist.log_prob(action_tensor)\n","        loss = -log_prob * reward_tensor\n","\n","        # Compute the gradients and update the model\n","        optimizer.zero_grad()\n","        loss.backward()\n","        optimizer.step()\n","\n","        state = next_state\n"],"metadata":{"id":"psdKrsXubQHV"},"execution_count":null,"outputs":[]}],"metadata":{"accelerator":"GPU","colab":{"provenance":[{"file_id":"11qohwNbxcNBvJBoKMvpJrtZy2LNWpc95","timestamp":1682809990606},{"file_id":"1W2sswuGKRxM4PCb83Zm2BbNnaoZPv4tk","timestamp":1682310416896}],"machine_shape":"hm"},"gpuClass":"premium","kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}